{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b61e5a0-d522-4947-b5da-f3b289d9ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riad_uddin\\Documents\\LEGOIO\\Basic_validation_project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1218894-ccab-4219-a7d1-598b88d0bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riad_uddin\\Documents\\LEGOIO\\Basic_validation_project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\riad_uddin\\.cache\\huggingface\\hub\\models--dslim--distilbert-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\") #dslim/bert-base-NER\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8644c69b-8c39-4019-ac95-6aef75a0300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    #tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens into cleaned text\n",
    "    #cleaned_text = ' '.join(filtered_tokens)\n",
    "    text=re.sub(r'\\n','', text)\n",
    "    text=re.sub(r'\\t','',text)\n",
    "    \n",
    "    return text#cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39782f94-4708-4ff5-b064-f45cf152a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DOCX file and extract text\n",
    "def extract_text_from_docx_file(uploaded_file):\n",
    "    # Use docx2txt to extract text from a DOCX file\n",
    "    return docx2txt.process(uploaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18f4f64-ee8a-4cbe-8624-2235fc9567bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>pop</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>province</th>\n",
       "      <th>abbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calais</td>\n",
       "      <td>Calais</td>\n",
       "      <td>45.165989</td>\n",
       "      <td>-67.242392</td>\n",
       "      <td>1781.5</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Houlton</td>\n",
       "      <td>Houlton</td>\n",
       "      <td>46.125517</td>\n",
       "      <td>-67.839720</td>\n",
       "      <td>6051.5</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Presque Isle</td>\n",
       "      <td>Presque Isle</td>\n",
       "      <td>46.793409</td>\n",
       "      <td>-68.002165</td>\n",
       "      <td>9466.0</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bar Harbor</td>\n",
       "      <td>Bar Harbor</td>\n",
       "      <td>44.387897</td>\n",
       "      <td>-68.204375</td>\n",
       "      <td>4483.5</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bangor</td>\n",
       "      <td>Bangor</td>\n",
       "      <td>44.801153</td>\n",
       "      <td>-68.778345</td>\n",
       "      <td>40843.0</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city    city_ascii        lat        lng      pop  \\\n",
       "0        Calais        Calais  45.165989 -67.242392   1781.5   \n",
       "1       Houlton       Houlton  46.125517 -67.839720   6051.5   \n",
       "2  Presque Isle  Presque Isle  46.793409 -68.002165   9466.0   \n",
       "3    Bar Harbor    Bar Harbor  44.387897 -68.204375   4483.5   \n",
       "4        Bangor        Bangor  44.801153 -68.778345  40843.0   \n",
       "\n",
       "                    country iso2 iso3 province  abbr  \n",
       "0  United States of America   US  USA    Maine   NaN  \n",
       "1  United States of America   US  USA    Maine   NaN  \n",
       "2  United States of America   US  USA    Maine   NaN  \n",
       "3  United States of America   US  USA    Maine   NaN  \n",
       "4  United States of America   US  USA    Maine   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('usa_cities.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc24558-b16f-4fea-a914-620c9a893b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=data['city'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a67e4b6-5719-4e0e-9871-f1ed87a80546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(words, chunk_size):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_word_count = 0\n",
    "\n",
    "    for word in words.split():\n",
    "        #print(word)\n",
    "        # Add the current word to the current chunk\n",
    "        current_chunk.append(word)\n",
    "        current_chunk_word_count += 1\n",
    "        # print(current_chunk)\n",
    "        # print(current_chunk_word_count)\n",
    "        # If the current chunk exceeds the desired word count\n",
    "        if current_chunk_word_count == chunk_size:\n",
    "            # Join the words in the current chunk to form a chunk string\n",
    "            #chunk_string = ' '.join(current_chunk)\n",
    "            chunks.append(current_chunk)\n",
    "            #print(chunks)\n",
    "\n",
    "            # Reset the current chunk and word count\n",
    "            current_chunk = []\n",
    "            current_chunk_word_count = 0\n",
    "    if current_chunk is not None:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50829d22-7e71-43d9-84b5-da772691cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location=read_words_from_file(\"cities.txt\")\n",
    "def removing_other_countries_applicant(docx_file_path):\n",
    "    # Extract text from DOCX file\n",
    "    #input_text = extract_text_from_docx_file(docx_file_path)\n",
    "    input_text=pdf_to_text(docx_file_path)\n",
    "    #print(input_text)\n",
    "    input_text=clean_text(input_text)\n",
    "    chunks=create_chunks(input_text,512)\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    results=[]\n",
    "    for chunk in chunks:\n",
    "        chunk=' '.join(chunk)\n",
    "        ner_results=nlp(chunk)\n",
    "        results.append(ner_results)\n",
    "\n",
    "    for result in results:\n",
    "        for x in result:\n",
    "            #print(x)\n",
    "            entity=x['entity']\n",
    "            # print(entity)\n",
    "            if entity in [\"B-LOC\", \"I-LOC\",\"B-ORG\",\"I-ORG\"]:\n",
    "                #print(x['word'])\n",
    "                if x['word'] in locations:\n",
    "                    print(entity)\n",
    "                # if data['city'].isin(x['word']).any():\n",
    "                    print(x['word'])\n",
    "                    return f\"Application {docx_file_path} is from the USA\"\n",
    "    return f\"Application {docx_file_path} is not from the USA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef16f8-2aeb-4efa-943f-8aff56eca8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
